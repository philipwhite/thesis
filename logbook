11 Sep '11
Note to self: a good attribute to analyze might involve looking at the arguments of a verb and seeing how many are lexical NPs and how many are referential forms. Recall that Du Bois says that no more than one lexical NP is preferred, in intransitives in the S position (of course), and in transitives and ditransitives in the D.O. position. This could be presented as fourteen attributes:
1 argument -> {s, S}
2 arguments -> {ao,aO,Ao,AO}
3 arguments -> {aio,aiO,aIo,aIO,Aio,AiO,AIo,AIO}

The values for each could be represented thus, where the variables are the number of occurences of that particular argument configuration:
Vs = sum(s)/(sum(s)+sum(S))
Vao = sum(ao)/(sum(ao)+sum(aO)+sum(Ao)+sum(AO))
and so forth

Sources:
Du Bois, John W. "Discourse and Grammar" from ed. Tomasello, Michael "The New Psychology of Language" Ch. 2 pp 48-87

3 Sep '11
No commit
Up next, figure out how tenses are represented in the syntactic parse trees and turn this into attributes. Let each verb tense be an attribute with the value being the number of times it appears in the text divide by the total number of verb usages. Restrict to finite verbs.


10 Aug '11
commit 17849cefb00c0082d9d6e5a4885ae83edc0eb6b6

Previously (about 10 days ago) wrote a function
train-deps/run-root-attr-trim-experiment that performs the following
experiment (from docs): This function makes an unpruned J48 tree using
the data in dataset, evaluates it using cross validation, then removes
the attribute from dataset that is the basis of the initial decision
in the tree and calls itself again, decrementing depth.
The purpose of this is to discover which features are apparently most
important in classification. This can likely also be done (though
probably with different results) using a classifier that can report
the most informative features automatically.

See log-addendum-002 for specifics.

29 Jul '11
3:14pm
commit 51b394f7ce93d8bed552b5e07119a35026db2719

Ran an experiment hoping to discover if some corpus files should be
dropped or otherwise treated specially due to small size. Initial
results seem to indicate no. The table below shows the results from 20
fold cross validation. See also log-addendum-001 for the list of files
used.  multilayer perceptron used as classifier with no special
options. Still need to investigate whether results can be improved by
pooling smaller files.

MinDep	Average correct over 10 runs
15	86.86131386861314
20	86.46616541353384
25 	85.88709677419355
35 	82.47422680412373
45 	82.49999999999999
60 	77.08333333333334


8:35am
commit 659bea22b18d4ff857aaa886b5797f1ce96a1aef

Training a decision tree  classifier (:decision-tree :c45 / J48 pruned
tree) on  a dataset consisting of  all L1-ES wricle texts  of levels 5
and 6 (79), all L1-ES micusp texts (8), and a sampling of L1-EN wricle
texts (48) yields a classifier which when evaluated with 10 fold cross
validation  yields a  success of  88.1481%.  The decision  tree is  as
follows:

nn <= 0.043394: es (78.0/1.0)
nn > 0.043394
|   predet <= 0.001776
|   |   advmod <= 0.034846
|   |   |   quantmod <= 0.001476
|   |   |   |   prt <= 0.00202: es (6.0)
|   |   |   |   prt > 0.00202: en (2.0)
|   |   |   quantmod > 0.001476: en (6.0)
|   |   advmod > 0.034846: en (40.0/1.0)
|   predet > 0.001776: es (3.0)

27   July  '11  Code   now  capable   of  doing   deps  relation-based
classification.   The   efficacy   of    this   has   not   yet   been
determined.  Relevent  code in  thesis.train-deps.  Next step,  choose
appropriate classifier algorith and test.

25 July '11 Added file  train_deps.clj to contain code for running the
deps-based  classification.  Using clj-ml  to  interface weka.   Began
initial test by  constructing a Weka data set  with as many attributes
as there are stanford dependencies  (50 some). I added a weka Instance
to the  data set for  each sample text.  I setup each attribute  to be
numeric, counting the number of times a particular dependency relation
is used in  a text (i.e. just the basic dep  such as "xsubj", ignoring
the  words that  are parameters),  dividing that  number by  the total
number of  dependencies and using  that as the attribute  value. Class
attribute  has  the  key  "L1"  with possible  values  "en"  or  "es".
thesis.train-deps/make-reln-dataset-with-samples  will  construct  the
weka dataset from  the supplied list of english  and spanish L1 micusp
files.   Starting  a   run  with  8  L1  spanish   and  9  L1  english
texts. Following run need to train a classifier.

20 July '11 10:05PM Renamed parser.clj to parse.clj along with package
thesis.parse.  This packages now  contains a  function parse-sentences
that takes  a list  of preprocessed text  and returns only  the parses
that  are complete sentences.  Also added  functions in  data.clj that
return the  preprocessed text. Next step  is to write  a function that
will create dependences for  these parses. Then use those dependencies
as features for WEKA.

2:29PM   Finished   a   preliminary   preprocessor  for   the   micusp
resource.  regexes  need improvement  as  some  documents  seem to  be
cutoff.  Also there  is  some  initial information  that  needs to  be
striped.

18 July '11 10:41PM Have been working on a preprocessor for the micusp
resource. Still in progress.  There are slight differences between the
files in terms of the page  headers that need to be stripped; mainly a
difference  of   newlines  but  possibly  more.   New  folder  code-nu
containing code that does batch converts from PDFs to utf8.

17 July '11
1:41PM
NB: MICUSP: Michigan Corpus of Upper-Level Student Papers
http://search-micusp.elicorpora.info/simple/

1:24PM Downloaded ice-canada corpus. Zip file (in data/ice-canada/) is
password  protected.  etc/ICElicence.doc  needs  to be  completed  and
emailed   after    July   '11    (researcher   is   out    of   town).
http://ICE-corpora.net/ice/download.htm

13 July  '11 9:32PM  Setup remote git  repository for this  project at
thesis@rivulus-sw.com:thesis

12 July '11 3:16PM
Initial entry. Up to this point, I have been
working in Clojure with the Stanford Parser (1.6.4 and now
1.6.7). Using the factored and PCFG parser I have generated
parse-trees and Stanford dependencies.
Yesterday spoke to Drs. Biava
and Smith regarding this project. Basically just explained my
intentions. Talked about finding corpora, the surprising paucity of
Spanish linguistics literature, etc.

I should provide a basic overview of my goals here:
Using parse trees, Stanford Dependencies, possibilty vocabulary, and a
machine learning package (likely WEKA) I want to create a system that
will look at an English text and attempt to determine if it was
written by a Spanish L1 or native English speaker. The problem here if
finding the features in trees etc. that will allow this
classification.

Immediate Plans:
Setup a Git repository? Today I emailed Dr. Smith regarding university
server space for this.
Stanford dependencies consists of "approximately 52 grammatical
relations" (SD manual). Try using these as features for a first
attempt at a classifications system. In other words, for a text just
count the number of each relation it has and see if that gives the
classifier something to work with.

Relevent Works:
-For the PCFG parser: Dan Klein and Christopher
D. Manning. 2003. Accurate Unlexicalized Parsing. Proceedings of the
41st Meeting of the Association for Computational Linguistics,
pp. 423-430. 
-For the factored parser: Dan Klein and Christopher
D. Manning. 2003. Fast Exact Inference with a Factored Model for
Natural Language Parsing. In Advances in Neural Information Processing
Systems 15 (NIPS 2002), Cambridge, MA: MIT Press, pp. 3-10.